---
title: "Problem Set 4"
author: "David Bacsik"
date: "May 12, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(reshape2)
library(gridExtra)
```


# Problem Set #4
# Due Date: Thursday, May 24th

### Problem 1
![](models.png)

Variables and values: 
G80: Gal80 Expression, Val(G80) = {G80^0^, G80^1^}
G4: Gal4 Expression, Val(G4) = {G4^0^, G4^1^}
G2: Gal2 Expression, Val(G2) = {G2^0^, G2^1^}

#### 1a
**Model 1 and Model 2 have different Bayesian network structures, and so they have different sets of parameters (θ). List all parameters and the CPTs in each of Model 1 and Model 2.**

Model 1:  
$\theta_1$ = {G80, G4|G80, G2|G4}

**CPT(Gal80):  **

| G80^0^ | G80^1^ |
| --------------- | --------------- |
| P(G80^0^)  | P(G80^1^)  |

**CPT(Gal4):  **

|   | G4^0^ | G4^1^ |
|---| --------------- | --------------- |
| **G80^0^** | P(G4^0^\|G80^0^)  | P(G4^1^\|G80^0^)  |
| **G80^1^** | P(G4^0^\|G80^1^)  | P(G4^1^\|G80^1^)  |

**CPT(Gal2):  **

|   | G2^0^ | G2^1^ |
|---| --------------- | --------------- |
| **G4^0^** | P(G2^0^\|G4^0^)  | P(G2^1^\|G4^0^)  |
| **G4^1^** | P(G2^0^\|G4^1^)  | P(G2^1^\|G4^1^)  |

Model 2:  
$\theta_2$ = {G80, G4, G2|G80,G4}

**CPT(Gal80):  **

| G80^0^ | G80^1^ |
| --------------- | --------------- |
| P(G80^0^)  | P(G80^1^)  |

**CPT(Gal4):  **

| G4^0^ | G4^1^ |
| --------------- | --------------- |
| P(G4^0^)  | P(G4^1^)  |

**CPT(Gal2):  **

|                                      | G2^0^                   | G2^1^ |
|------------                         | ---------------                   | --------------- |
| **G80^0^, G4^0^** | P(G2^0^\|G80^0^, G4^0^)  | P(G2^1^\|G80^0^, G4^0^)  |
| **G80^0^, G4^1^** | P(G2^0^\|G80^0^, G4^1^)  | P(G2^1^\|G80^0^, G4^1^)  |
| **G80^1^, G4^0^** | P(G2^0^\|G80^1^, G4^0^)  | P(G2^1^\|G80^1^, G4^0^)  |
| **G80^1^, G4^1^** | P(G2^0^\|G80^1^, G4^1^)  | P(G2^1^\|G80^1^, G4^1^)  |

#### 1b
**Say that we are given the gene expression data D measuring binary expression levels of the 3 genes (Gal80, Gal4 and Gal2) across 112 samples. Write down the likelihood function L(D|θ) for Model 1 and Model 2.**

L($\theta$ : D) = P(D|$\theta$)

Model 1:  
$\theta_1$ = {G80, G4|G80, G2|G4}  
$L_1(\theta_1:D) = P(D|\theta_1) = \displaystyle\prod_{i=1}^{112} P(G80 = G80_i) \cdot P(G4 = G4_i | G80_i) \cdot P(G2 = G2_i|G4_i)$

Model 1:  
$\theta_2$ = {G80, G4, G2|G80,G4}
$L_2(\theta_2:D) = P(D|\theta_2) = \displaystyle\prod_{i=1}^{112} P(G80 = G80_i) \cdot P(G4 = G4_i) \cdot P(G2 = G2_i|G80_i, G4_i)$

#### 1c QUESTION
**Write down the maximum likelihood estimation (MLE) solutions in Model 1 and Model 2.**

#### 1d
**Download the binary expression data from https://sites.google.com/a/cs.washington.edu/genome560-spr18/disc-gal80-gal4-gal2.txt?attredirects=0&d=1, and implement the code that computes the log-likelihood function log (L) for Model 1 and Model 2.**

```{r}

gal_data = read.table('disc-gal80-gal4-gal2.txt', sep='\t', strip.white=TRUE, stringsAsFactors = TRUE)

gal_data = t(gal_data)

colnames(gal_data) = gal_data[1,]
row.names(gal_data) = NULL

gal_data = gal_data[-1,-1]

gal_data = data.frame(gal_data)

gal_data = lapply(gal_data, trimws)

gal_data = lapply(gal_data, as.numeric)

gal_data = data.frame(gal_data)
```

```{r}
gal80.cpt = prop.table(table(gal_data[c('Gal80')]))

gal4.cpt = prop.table(table(gal_data[c('Gal4')]))

gal4.gal80.cpt = prop.table(table(gal_data[c('Gal80','Gal4')]))

gal2.gal4.cpt = prop.table(table(gal_data[c('Gal4','Gal2')]))

gal2.gal4gal80.cpt = prop.table(table(gal_data[c('Gal80', 'Gal4', 'Gal2')]))

term_prob_1 = function(x){
  g80 = gal_data[x,]$Gal80
  g4 = gal_data[x,]$Gal4
  g2 = gal_data[x,]$Gal2
  
  p.g80 = as.numeric(gal80.cpt[as.character(g80)])
  p.gal4.gal80 = as.numeric(gal4.gal80.cpt[as.character(g80),as.character(g4)])
  p.gal2.gal4 = as.numeric(gal2.gal4.cpt[as.character(g4),as.character(g2)])
  
  joint = p.g80 * p.gal4.gal80 * p.gal2.gal4
  return(joint)
}

term_prob_2 = function(x){
  g80 = gal_data[x,]$Gal80
  g4 = gal_data[x,]$Gal4
  g2 = gal_data[x,]$Gal2
  
  p.g80 = as.numeric(gal80.cpt[as.character(g80)])
  p.gal4 = as.numeric(gal4.cpt[as.character(g4)])
  p.gal2.gal4gal80 = as.numeric(gal2.gal4gal80.cpt[as.character(g80), as.character(g4),as.character(g2)])
  
  joint = p.g80 * p.gal4* p.gal2.gal4gal80
  return(joint)
}

prod_1 = 1
prod_2 = 1
for (i in  1:nrow(gal_data)){
  j_1 = term_prob_1(i)
  prod_1 = prod_1 * j_1
  j_2 = term_prob_2(i)
  prod_2 = prod_2 * j_2
}

print('log likelihood of model 1:')
print(log(prod_1))

print('log likelihood of model 2:')
print(log(prod_2))

```

#### 1e
**Select between Model 1 and Model 2 based on the results in part (d).**

The log likelihood of model 2 is greater (less negative). Therefore, I believe model 2 better reflects the data.

### Problem 2
**Here, we will continue the R exercise in Lecture #14. The goal of this exercise is to understand the impact of hyperparameters α and β in a Bernoulli experiment (e.g., Thumbtack example). We will do that by comparing the shape of the distribution between the likelihood P(D|p) and the posterior P(p|D). We assume that our prior belief is that we get the same number of heads and tails (i.e., p = 0.5).**

```{r}
likelihood <- function(p, nh, nt){
  l = p^nh * (1-p)^nt
  return(l)
}

posterior = function(p, nh, nt, alpha, beta){
  prior = dbeta(p, alpha, beta)
  like = likelihood(p, nh, nt)
  post = (1/prior)*like
  return(post)
}

```

**Consider the following sets of nH, nT , α, and β. Plot the likelihood and posterior functions over varying p in [0, 1] and compare multiple plots. What are the MLE and MAP estimations of p in each case?**

```{r}
p = seq(0, 1, 0.001)

likepost = function(p, nh, nt, alpha, beta){
  likes = likelihood(p, nh, nt)
  posts = posterior(p, nh, nt, alpha, beta)
  
  ld = data.frame(p, likes, posts)
  ld
  
  max_like = p[which.max(likes)]
  print('The maximum likelilhood estimate of p is:')
  print(max_like)
  
  max_map = p[which.max(posts)]
  print('The maximum posterior estimate of p is:')
  print(max_map)
  
  plot_like = ggplot(ld, aes(x=p, y=likes)) + geom_point(col='seagreen3') + labs(x='p', y='likelihood')
  plot_post = ggplot(ld, aes(x=p, y=posts)) + geom_point(col='deeppink3') + labs(x='p', y='posterior probability')
  
  grid.arrange(plot_like, plot_post, ncol=1)
}

```

#### 1a
**Say that nH = 100, nT = 50, α = 5, and β = 5**

```{r}
nh = 100
nt = 50
alpha = 5
beta = 5

likepost(p, nh, nt, alpha, beta)
```

#### 1b
Say that nH = 100, nT = 50, α = 30, and β = 30. (Optional: Describe the difference of the results between (a) and (b) and the reason for the difference.)

In this case, we are adding more pseudocounts, indicating a stronger prior.

```{r}
nh = 100
nt = 50
alpha = 30
beta = 30

likepost(p, nh, nt, alpha, beta)
```


#### 1c
Say that nH = 10, nT = 5, α = 30, and β = 30. (Optional: Describe the difference of the results between (b) and (c) and the reason for the difference.)

Now, we have a really strong prior. We still have a high beta and alpha value (psuedocounts). We have fewer observations in our data, making the absolute value of the likihood lower. So, our prior almost totally will drive the posterior probability distribution. In this case, the prior is that the number of counts will be very high, so the highest value in our data set gets the greatest confidence.

```{r}
nh = 10
nt = 5
alpha = 30
beta = 30

likepost(p, nh, nt, alpha, beta)
```


### Problem 3
**We are given the genotype and phenotype data from 334 mouse individuals. The genotype data measure binary genotype values of 1333 genetic markers for each mouse, and the phenotype data measure the normalized blood cholesterol levels. Given these data, we want to find the quantitative trait loci (QTLs) that contribute to elevated cholesterol level.**

**We will perform a single predictor regression. This means that we will model the phenotype based on the linear regression model with only one marker . For each genetic marker j, we model the measurement of blood cholesterol level Y as a linear combination of an intercept term and the genotype value Xj : Y = β0 + βXj.**

```{r}
genotype = read.table('genotype.txt', sep='\t', strip.white=TRUE, stringsAsFactors = TRUE)

genotype = t(genotype)

colnames(genotype) = genotype[1,]
row.names(genotype) = NULL

genotype = genotype[-1,-1]

genotype = data.frame(genotype)

genotype = lapply(genotype, trimws)

genotype = lapply(genotype, as.numeric)

genotype = data.frame(genotype)

phenotype = read.table('phenotype.txt', sep='\t', strip.white=TRUE, stringsAsFactors = TRUE)

phenotype = t(phenotype)

colnames(phenotype) = phenotype[1,]
row.names(phenotype) = NULL

phenotype = phenotype[-1,-1]

phenotype = data.frame(phenotype)

phenotype = lapply(phenotype, trimws)

phenotype = lapply(phenotype, as.numeric)

phenotype = data.frame(phenotype)

chol_data = data.frame(genotype, phenotype)

```

#### 3a
**Use “lsfit” or “lm” (Method II in R exercise of Lecture 15) to compute the β values. Implement a function that computes for each marker i, the squared error of prediction of Y by using the β values from a linear regression fit.**

```{r}

marker.sse = function(m){
  if(m != 'phenotype'){
    x = chol_data[[m]]
    out = lm(chol_data$phenotype ~ x)
    return(anova(out)[['Sum Sq']][2])
  }
  else{
    return(NULL)
  }
}

sse = sapply(names(chol_data), marker.sse)
sse = unlist(sse)
```

**What is the minimum squared error (SSE) across all 1333 genetic markers? What is the marker that has the minimum squared error?**
```{r}
min(sse)

sse[which.min(sse)]
```

#### 3b
**What is the maximum SSE across all 1333 markers? What is the marker that has the maximum squared error?**
```{r}
max(sse)

sse[which.max(sse)]
```

#### 3c
**Compute the coefficient of determination (R2) for each of 1333 markers. Plot the R2 values (y-axis) across all 1333 markers (x-axis).**

```{r}
marker.r2 = function(m){
  if(m != 'phenotype'){
    x = chol_data[[m]]
    out = lm(chol_data$phenotype ~ x)
    return(summary(out)$r.squared)
  }
  else{
    return(NULL)
  }
}

r2 = sapply(names(chol_data), marker.r2)
r2 = unlist(r2)

r2_data = data.frame(r2)

r2.plot = ggplot(r2_data, aes(x=1:length(row.names(r2_data)), y=r2)) + geom_point(color='navy') + labs(x='Marker Number', y='R^2')
r2.plot
```

#### 3d
**What is the R2 value of the marker you found in (a)? What is the R2 value of the marker you found in (b)?**

In a, I found that the minimum squared error was found with Marker #847; its squared error was 287.3. This means the linear model for this marker fit the data the best, so difference between the prediction and the actual value (the error) was small. The R2 value for this marker is relatively high:
```{r}
r2[which.min(sse)]
```

In b, I found that the maximum squared error was found with Marker 598; its squared error was 328. This means the linear model for this marker fit the data the worst, so the difference between the prediction and the actual value was large. The R2 for this marker was low:
```{r}
r2[which.max(sse)]
```

#### 3e
**Generate a scatter plot of Y and X (e.g., slides 19-22 in Lecture 15) and add a fitted linear regression line for each of the markers you found in (a) and (b). Descibe how they are different in terms of goodness of fit.**

**Best Model: Marker #847**

Marker 847 provided the best linear model of cholesterol.  
```{r}
m847 = lm(chol_data$phenotype ~ chol_data$Marker_847)

summary(m847)
```

When plotting Marker 847 (x-axis) against cholesterol, you can see mice with Marker 847 tend to have higher cholesterol levels than mice without Marker 847. The line is positive, and runs through the center of these two clusters.
```{r}
p847 = ggplot(chol_data, aes(x=Marker_847, y=phenotype)) + geom_point(color='seagreen3') + labs(x='Marker 847 Status', y='Cholesterol') + geom_abline(slope = 0.82479, intercept = -0.19261, color='aquamarine4', size=1.5)
p847

```

**Worst Model: Marker #598**

Marker 598 provided the worst linear model of cholesterol.  
```{r}
m598 = lm(chol_data$phenotype ~ chol_data$Marker_598)

summary(m598)
```

When plotting Marker 598 (x-axis) against cholesterol, you can see that it does not make a difference whether mice have Marker 598 or not; both sets of mice look like they are drawn from the same population, with a cholesterol around 0 (averaged against all samples). The line is flat and runs through the center of both clusters.
```{r}
p598 = ggplot(chol_data, aes(x=Marker_598, y=phenotype)) + geom_point(color='deeppink3') + labs(x='Marker 598 Status', y='Cholesterol') + geom_abline(slope = -2.142*(10^-5), intercept = 1.478*(10^-5), color='indianred4', size=1.5)
p598

```

