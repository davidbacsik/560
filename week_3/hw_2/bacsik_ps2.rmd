---
title: "PS_2"
author: "David Bacsik"
date: "April 22, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem Set 2
## David Bacsik
## Due Date: Thursday, April 26, 2018

### Probelm 1
You have 10 lymphoblastic cell lines, as well as transcript counts for a gene of interest at baseline and after stimulation.

```{r}
individual = c(1,2,3,4,5,6,7,8,9,10)
baseline = c(-0.24, 0.25, 1.12, -0.06, 0.46, 0.17, 0.02, 1.10, 0.55, 0.98)
stimulated = c(1.74, 2.1, 1.65, 2.65, 3.11, 2.31, 1.87, 3.21, 2.19, 1.75)
stim_data = data.frame(individual, baseline, stimulated)
```

#### 1a
*Perform a one sample t-test comparing baseline to zero.*
The null hypothesis is that the baseline mean is not different from zero.
The althernative hypothesis is that the baseline mean is different from zero.

```{r}
t.test(stim_data$baseline, mu = 0)
```

The p value here is less than 0.05, so I am concluding that the true mean is different from zero.

#### 1b
Now, I would like to see if the transcript counts change after stimulation. To test this, I will see if the means of the baseline and stimulated values are different.

*I will conduct a two-sample t-test, two sided.* Because each row represents the same sample stimulated and unstimulated, this is a paired data set. I will conduct a paired t-test.

The null hypothesis is that the means are the same.
The althernative hypthosis is that the means are not the same.

```{r}
t.test(x=stim_data$baseline, y=stim_data$stimulated, paired=TRUE)
```

The p value is much less than 0.05, so I am confident that the means of the two groups are not equal. From this, I conclude that stimulation does change the transcript count.

#### 1c
Rather than performing a classic paired t-test, I will compute the difference between each sample's stimulated and unstimulated states by hand. To do this, I will subtract the baseline value from the stimulated for each sample.

```{r}
stim_data$diff = stim_data$stimulated - stim_data$baseline

stim_data
```

Then, to see whether or not the means in the stimulated and baseline groups are the same, we simply perform a one-sample t-test, assuming that the mean of the difference between each sample is equal to 0.
I expect the p-value for this test to be the same as the paired t-test, as it is fundamentally asking the same question.

```{r}
t.test(stim_data$diff, mu=0)
```
Indeed, the p-value is the same and the mean difference between the stimulated and unstimulated conditions is found to be the same value (1.823). One produced a positive mean and the other a negative mean, but this is simply because of how the conditions were orderd.

### Problem 2
#### 2a
*Generate two groups of n=5 observations, each from a standard normal distribution.*
Want it to reproducible, so going to set seed = 1

```{r}
set.seed(1)

group_1 = rnorm(5)
group_2 = rnorm(5)

group_1
group_2
```

*Perform a t-test to test H0 : µ1 = µ2 vs H1 : µ1 != µ2 and record the resulting p-value.*

```{r}
t.test(x=group_1, y=group_2)

```

The p-value from this test 0.99.

*Now repeat these steps 2000 times and calculate the number of times you would have rejected the null hypothesis with α = 0.05.*

```{r}
set.seed(1)

random_5 = function() {
  t.test(x=rnorm(5),y=rnorm(5))$p.value
}

pvalues_5 = replicate(2000, random_5())
```

I would reject the null hypothesis whenever the p-value is less than 0.05. This is:
```{r}

sum(pvalues_5 < 0.05)
```

Taking random samples from a normal distribution, I would expect about 5% (0.05) of samples to have means that are sufficiently different to generate a p-value that is less than or equal to 0.05. That is, about 5% of the time, by chance, even sampling from the same population, you would see a difference that extreme.

For 2000 samples, this is: 2000 * 0.05, or 100 samples. Our result (83) is close to that value.

*If the alpha is dropped to 0.01*, I would expect 1 percent, or 20 samples, to cross this threshold.

```{r}

sum(pvalues_5 < 0.01)
```

In our simulation, I found 17 samples crossed the threshold. Again, this is close.

#### 2b
*Repeat problem a, but now change the sample size to 50 and then to 500. Do the results change? Why or why not?*

First, I will test a sample size of 50:
```{r}
set.seed(1)

random_50 = function() {
  t.test(x=rnorm(50),y=rnorm(50))$p.value
}

pvalues_50 = replicate(2000, random_50())
```

Again, I will only expect about 5% (~100) of the samples to result in means extreme enough to generate a p value less than 0.05.

```{r}

sum(pvalues_50 < 0.05)
```

In this case, exactly 100 had p-values less than 0.05. This is in line with expectation.

Testing an alpha of 0.01, I would expect about 20 samples to result in means extreme enough to generate a p value this small.

```{r}

sum(pvalues_50 < 0.01)
```

In this case, 22 had p-values less than 0.01. Again, this is in line with expectation.

Finally, I will test a sample size of 500.

```{r}
set.seed(1)

random_500 = function() {
  t.test(x=rnorm(500),y=rnorm(500))$p.value
}

pvalues_500 = replicate(2000, random_500())

sum(pvalues_500 < 0.05)

sum(pvalues_500 < 0.01)
```

Again, the values are as expected (103 and 17).

Because the distribution of the mean is normal, changing the sample size does not change the number of sample means which are extreme, surpassing the p-value threshold.

#### 2c
*Repeat problem a, but now change the mean of one of the groups to be µ = 1. What did you expect would happen? Summarize your results.*

Preivously the two groups were drawn from the same population. They both had means of 0 and standard deviations of 1.

Now, the two groups are drawn from different populations. Much more frequently, we would expect the difference between the means to be extreme (by the standards of a normal distribution). Therefore, I expect more samples to produce p-values that pass the threshold.

```{r}
set.seed(1)

random_5_twopops = function() {
  t.test(x=rnorm(5),y=rnorm(5, mean=1))$p.value
}

pvalues_5_twopops = replicate(2000, random_5_twopops())

sum(pvalues_5_twopops < 0.05)

sum(pvalues_5_twopops < 0.01)
```

As expected, the number of samples that have means sufficently different to produce a low p-value is 544, much greater than the 100 that would be expected by chance if the two samples were drawn from the same population.

#### 1d
*Repeat problem b, but now change the mean of one of the groups to be µ = 1. What did you expect would happen? Summarize your results.*

Because these samples are drawn from two different populations, I expect more than 100 samples to produce a low p-value. However, as in problem b, I do not expected the number of samples that produce a low p-value to change as sample size changes.

```{r}
set.seed(1)

random_50_twopops = function() {
  t.test(x=rnorm(50),y=rnorm(50, mean = 1))$p.value
}

pvalues_50_twopops = replicate(2000, random_50_twopops())

sum(pvalues_50_twopops < 0.05)

sum(pvalues_50_twopops < 0.01)

random_500_twopops = function() {
  t.test(x=rnorm(500),y=rnorm(500, mean = 1))$p.value
}

pvalues_500_twopops = replicate(2000, random_500_twopops())

sum(pvalues_500_twopops < 0.05)

sum(pvalues_500_twopops < 0.01)
```

#ASK

The results indicate that almost all the samples in the 50 sample size test, and all of the samples in the 500 sample size test produce extremely low p-values. This is not the result I expected. I suppose intuitively, as you get a larger sample, you get more confident that they are from different populations... Why is this different when you draw from a single population? Is there something I could read about this?

#### 2e

#ASK
Plot the results from 2a-d on a single plot. This is sketched out on a notebook on your desk.

Also, do a quick ggplot tutorial so you can plot this easily and comfortably. No more than 30 minutes.

### Problem 3.
