---
title: "PS_2"
author: "David Bacsik"
date: "April 22, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
```

# Problem Set 2
## David Bacsik
## Due Date: Thursday, April 26, 2018

### Probelm 1
You have 10 lymphoblastic cell lines, as well as transcript counts for a gene of interest at baseline and after stimulation.

```{r}
individual = c(1,2,3,4,5,6,7,8,9,10)
baseline = c(-0.24, 0.25, 1.12, -0.06, 0.46, 0.17, 0.02, 1.10, 0.55, 0.98)
stimulated = c(1.74, 2.1, 1.65, 2.65, 3.11, 2.31, 1.87, 3.21, 2.19, 1.75)
stim_data = data.frame(individual, baseline, stimulated)
```

#### 1a
**Perform a one sample t-test comparing baseline to zero.**
The null hypothesis is that the baseline mean is not different from zero.
The althernative hypothesis is that the baseline mean is different from zero.

```{r}
t.test(stim_data$baseline, mu = 0)
```

The p value here is less than 0.05, so I am concluding that the true mean is different from zero.

#### 1b
Now, I would like to see if the transcript counts change after stimulation. To test this, I will see if the means of the baseline and stimulated values are different.

**I will conduct a two-sample t-test, two sided.** Because each row represents the same sample stimulated and unstimulated, this is a paired data set. I will conduct a paired t-test.

The null hypothesis is that the means are the same.
The althernative hypthosis is that the means are not the same.

```{r}
t.test(x=stim_data$baseline, y=stim_data$stimulated, paired=TRUE)
```

The p value is much less than 0.05, so I am confident that the means of the two groups are not equal. From this, I conclude that stimulation does change the transcript count.

#### 1c
Rather than performing a classic paired t-test, I will compute the difference between each sample's stimulated and unstimulated states by hand. To do this, I will subtract the baseline value from the stimulated for each sample.

```{r}
stim_data$diff = stim_data$stimulated - stim_data$baseline

stim_data
```

Then, to see whether or not the means in the stimulated and baseline groups are the same, we simply perform a one-sample t-test, assuming that the mean of the difference between each sample is equal to 0.
I expect the p-value for this test to be the same as the paired t-test, as it is fundamentally asking the same question.

```{r}
t.test(stim_data$diff, mu=0)
```
Indeed, the p-value is the same and the mean difference between the stimulated and unstimulated conditions is found to be the same value (1.823). One produced a positive mean and the other a negative mean, but this is simply because of how the conditions were orderd.

### Problem 2
#### 2a
**Generate two groups of n=5 observations, each from a standard normal distribution.**
Want it to reproducible, so going to set seed = 1

```{r}
set.seed(1)

group_1 = rnorm(5)
group_2 = rnorm(5)

group_1
group_2
```

**Perform a t-test to test H0 : µ1 = µ2 vs H1 : µ1 != µ2 and record the resulting p-value.**

```{r}
t.test(x=group_1, y=group_2)

```

The p-value from this test 0.99.

**Now repeat these steps 2000 times and calculate the number of times you would have rejected the null hypothesis with α = 0.05.**

```{r}
set.seed(1)

random_5 = function() {
  t.test(x=rnorm(5),y=rnorm(5))$p.value
}

pvalues_5 = replicate(2000, random_5())
```

I would reject the null hypothesis whenever the p-value is less than 0.05. This is:
```{r}

rej_5.05 = sum(pvalues_5 < 0.05)

rej_5.05
```

Taking random samples from a normal distribution, I would expect about 5% (0.05) of samples to have means that are sufficiently different to generate a p-value that is less than or equal to 0.05. That is, about 5% of the time, by chance, even sampling from the same population, you would see a difference that extreme.

For 2000 samples, this is: 2000 ** 0.05, or 100 samples. Our result (83) is close to that value.

**If the alpha is dropped to 0.01**, I would expect 1 percent, or 20 samples, to cross this threshold.

```{r}

rej_5.01 = sum(pvalues_5 < 0.01)

rej_5.01

```

In our simulation, I found 17 samples crossed the threshold. Again, this is close.

#### 2b
**Repeat problem a, but now change the sample size to 50 and then to 500. Do the results change? Why or why not?**

First, I will test a sample size of 50:
```{r}
set.seed(1)

random_50 = function() {
  t.test(x=rnorm(50),y=rnorm(50))$p.value
}

pvalues_50 = replicate(2000, random_50())
```

Again, I will only expect about 5% (~100) of the samples to result in means extreme enough to generate a p value less than 0.05.

```{r}

rej_50.05 = sum(pvalues_50 < 0.05)

rej_50.05
```

In this case, exactly 100 had p-values less than 0.05. This is in line with expectation.

Testing an alpha of 0.01, I would expect about 20 samples to result in means extreme enough to generate a p value this small.

```{r}

rej_50.01 = sum(pvalues_50 < 0.01)

rej_50.01
```

In this case, 22 had p-values less than 0.01. Again, this is in line with expectation.

Finally, I will test a sample size of 500.

```{r}
set.seed(1)

random_500 = function() {
  t.test(x=rnorm(500),y=rnorm(500))$p.value
}

pvalues_500 = replicate(2000, random_500())

rej_500.05 = sum(pvalues_500 < 0.05)
rej_500.05

rej_500.01 = sum(pvalues_500 < 0.01)
rej_500.01
```

Again, the values are as expected (103 and 17).

Because the distribution of the mean is normal, changing the sample size does not change the number of sample means which are extreme, surpassing the p-value threshold.

#### 2c
**Repeat problem a, but now change the mean of one of the groups to be µ = 1. What did you expect would happen? Summarize your results.**

Preivously the two groups were drawn from the same population. They both had means of 0 and standard deviations of 1.

Now, the two groups are drawn from different populations. Much more frequently, we would expect the difference between the means to be extreme (by the standards of a normal distribution). Therefore, I expect more samples to produce p-values that pass the threshold.

```{r}
set.seed(1)

random_5_twopops = function() {
  t.test(x=rnorm(5),y=rnorm(5, mean=1))$p.value
}

pvalues_5_twopops = replicate(2000, random_5_twopops())

rej_5_twopops.05 = sum(pvalues_5_twopops < 0.05)
rej_5_twopops.05

rej_5_twopops.01 = sum(pvalues_5_twopops < 0.01)
rej_5_twopops.01
```

As expected, the number of samples that have means sufficently different to produce a p value less than 0.05 is 544, much greater than the 100 that would be expected by chance if the two samples were drawn from the same population.

#### 1d
**Repeat problem b, but now change the mean of one of the groups to be µ = 1. What did you expect would happen? Summarize your results.**

Because these samples are drawn from two different populations, I expect more than 100 samples to produce a low p-value. However, as in problem b, I do not expected the number of samples that produce a low p-value to change as sample size changes.

```{r}
set.seed(1)

random_50_twopops = function() {
  t.test(x=rnorm(50),y=rnorm(50, mean = 1))$p.value
}

pvalues_50_twopops = replicate(2000, random_50_twopops())

rej_50_twopops.05 = sum(pvalues_50_twopops < 0.05)
rej_50_twopops.05

rej_50_twopops.01 = sum(pvalues_50_twopops < 0.01)
rej_50_twopops.01

random_500_twopops = function() {
  t.test(x=rnorm(500),y=rnorm(500, mean = 1))$p.value
}

pvalues_500_twopops = replicate(2000, random_500_twopops())

rej_500_twopops.05 = sum(pvalues_500_twopops < 0.05)
rej_500_twopops.05

rej_500_twopops.01 = sum(pvalues_500_twopops < 0.01)
rej_500_twopops.01
```

This is not the result I expected. Unlike the 5 sample size test, almost all the samples in the 50 sample size test, and all of the samples in the 500 sample size test produce extremely low p-values. As you get a larger sample, the denominator in the t-test gets larger and larger, so for the same difference of means, your t-statistic gets larger and larger. This means that you are more and more confident that the difference in means is not due to chance as your sample size gets larger.

#### 2e

Plot the results from parts a-d:
**Try to produce one plot that summarizes the results from all four problems, showing how your power (probability of rejecting the null) changes based on sample size and the difference in means.**

To do this, I will make a plot which compares sample size on the x-axis to probability of rejecting the null on the y-axis. I will make one plot for alpha = 0.05, and one plot for alpha = 0.01.

First, I must combine the data from parts a-d into a single dataframe.

```{r}
delta_mean = c('0',0,0,0,0,0,1,1,1,1,1,1)
alpha = c('alpha=0.05','alpha=0.01','alpha=0.05','alpha=0.01','alpha=0.05','alpha=0.01','alpha=0.05','alpha=0.01','alpha=0.05','alpha=0.01','alpha=0.05','alpha=0.01')
sample_size = c(5,5,50,50,500,500,5,5,50,50,500,500)
prob_rej = c(rej_5.05, rej_5.01, rej_50.05, rej_50.01, rej_500.05, rej_500.01, rej_5_twopops.05, rej_5_twopops.01, rej_50_twopops.05, rej_50_twopops.01, rej_500_twopops.05,rej_500_twopops.01)

power_data = data.frame(delta_mean, alpha, sample_size, prob_rej)

power_data
```

Next, I will generate my plots.

```{r}
g = ggplot(power_data, aes(x=sample_size, y=prob_rej)) +
  geom_point(aes(col=delta_mean)) +
  geom_smooth(method='loess', aes(col=delta_mean), se=FALSE)

g_facet = g + facet_wrap( ~ alpha) +
   labs(title="Power of t-test by sample size", subtitle='2000 random samplings', y="Power (probability of rejecting null)", x="Sample Size", color='Delta of Means')
   
suppressWarnings(plot(g_facet))
```

When the difference of populations means is 0 (that is, samples are drawn from the same population), sample size does not matter, and the probability of rejecting the null is equal to that expected by chance.

When the difference of population means is 1 (samples are drawn from different populations), sample size increases the power and confidence is calling two distinct populations. Going from 5 to 50 drastically increases power, but then the it is saturated, as going to 500 adds little confidence.

This is true for both alpha = 0.05 and alpha = 0.01.

### Problem 3.
An expensive private school also asks for donations.
First, I will put the data into a dataframe.

Year is the year a class will graduate.
Donated is the number of parents from that class who donated to the school.
Total parents is the total number of parents for a given class.

Also, want to ultimately generate a contingency table, so having the number of parents that didn't donate is a useful value as well.


```{r}
year = c(2009:2021)
donated = c(35,42,39,37,38,35,32,19,22,20,17,34,28)
total_parents = c(51,56,70,60,53,54,53,27,31,30,32,34,32)
school_data = data.frame(year, donated, total_parents)
school_data$no_donate = total_parents-donated

school_data
```

#### 3a
**Now, I'm going to do a chi squared test on the data.** I will feed a contingency table, comparing proportions of donated and no-donate parents each year.

```{r}
library(dplyr)

chisq.test(select(school_data, donated, no_donate))

```

The results indicate that the proportions of parents donating are not the same each year. The p value is less than 0.01, so we reject the null hypothesis that proportions are independent from year.

#### 3b
**Is this to be done as one-tailed or two-tailed? Why? (Does a low Chi-square mean a departure from the expected proportions?)**

The chi-square test is a one-tailed test. A particularly low chi square value reflects independence of groups; that is, each group behaves similarly as they would if all were sub-samples of a single population. Only large chi-square values indicate departure from the expected value under the assumption of independence.

#### 3c
Make the test focus more on the question at issue, and not waste effort on detecting whether there are differences that do not represent a long term trend.

As a reminder, the question is whether parents burn out on donating over time. If this is the case, you might expect the proportion of donating parents to decrease over time.

A very simple way to test this assumption is to split the classes in half. In our data, classes are listed by graduation year. Young students would be expected to have parents who donate at a higher rate than older students.

In our data, classes are listed by year. High values represent young students, and low values represent old students. I will collapse the data into two rows - young and old. There are 13 years listed, so I will put 7 in the old group and 6 in the young group.

```{r}
group = c('old','young')

donated.lumped = c(sum(school_data$donated[1:7]), sum(school_data$donated[8:13]))

no_donate.lumped = c(sum(school_data$no_donate[1:7]), sum(school_data$no_donate[8:13]))

school.lumped = data.frame(group, donated.lumped, no_donate.lumped)

school.lumped

chi.school.lumped = chisq.test(select(school.lumped, donated.lumped, no_donate.lumped))

chi.school.lumped

chi.school.lumped$residuals
```

The results indicate a p-value of 0.01. This indicates that both groups do not donate at the same rate.

To see which group donates more than expected by indenpendence, and which group donates less, we can look at the residuals. The old group donates less than the young group, and less that would be expected by chance if they all donate at the same rate.

#### 3d
**What is the effect on the Chi-square test, on average, if some of the parents have two (or more) children in more than one grade, and thus are listed as donating (or not) in both of those grades on the basis of the same donation or the same non-donation?**

The chi-square test is assumed to have indepenent observations, and having more than one child per parent violates this assumption.

The chi-square test tests the null hypothesis that each group is sampled from the same population, and proportions are therefore independent of group. Having parents in more than one group moves the truth closer to this hypothesis; that is, group does not matter when it is the same parents. Therefore, intuitively, I would think that having parents with multiple children in different grades would artificially deflate the chi-square value and decrease the power of your test to find differences between grades.

### Problem 4

#### 4a
** Generate two groups of observations, each with n = 5 form a standard normal distribution.**

```{r}
set.seed(100)

group_1 = rnorm(5)
group_2 = rnorm(5)

group_1
group_2
```

**Perform a t-test for a difference in location for these two groups, and record the p-value.**
```{r}

t.test(group_1,group_2)

p.t = t.test(group_1,group_2)$p.value

```

**Now perform a Wilcoxon Rank-Sum test for a difference in location for these two groups and record the p-value.**

```{r}

wilcox.test(group_1,group_2)

p.w = wilcox.test(group_1,group_2)$p.value

```

The two tests produced similar results for two samples drawn from the same normally distributed population.

#### 4b

**Repeat this process 1,000 times.**
First, I need to generate the samples to compare these two tests.  
I will store the samples in a matrix. It will have 1,000 rows and 10 columns. The first five columns will represent group_1 and the second five columns will represent group_2.
```{r}
set.seed(100)

big.matrix.1 = replicate(1000, rnorm(5))
big.matrix.2 = replicate(1000, rnorm(5))

t_x = function(abc) {
  t.test(x = big.matrix.1[,abc], y = big.matrix.2[,abc])$p.value
}

p_vals.t = lapply(1:1000, t_x)

w_x = function(def) {
  wilcox.test(x = big.matrix.1[,def], y = big.matrix.2[,def])$p.value
}

p_vals.w = lapply(1:1000, w_x)

p_vals = data.frame(t=as.matrix(p_vals.t), w=as.matrix(p_vals.w))

ggplot(p_vals, aes(x=unlist(t),y=unlist(w))) + geom_point()
```

**At a level of α = 0.05, how often do you reject the null hypothesis for each test? What can you conclude about the type 1 error rate of each test, given these results? Which test is more conservative and why?**

```{r}
p_vals.tl = p_vals$t < 0.05
sum(p_vals.tl)

p_vals.wl = p_vals.w < 0.05
sum(p_vals.wl)
```

You reject the null hypothesis 45 times with the t test, and 30 times with the wilcoxan rank sum. Since all of these samples are drawn from the same random population, all of these rejections are type 1 errors. A type 1 error (rejecting the null hypothesis when it is true) occurs less often with a Wilcoxan than with a t-test. This means that the Wilcoxan is more conservative. Because you do not look at the magnitude of the difference between sample items--only their rank--you are less likely to be thrown off by randomly sampling a large difference in the mean, but you give up some power to detect differences in the mean if the populations are truly different.

#### 4c
First, I will make two samples of n=5 drawn from a random normal distribution. One population will have a mean of 0, while the other will have a mean of 1.

```{r}
set.seed(10)

group_1 = rnorm(5, mean=0)
group_2 = rnorm(5, mean=1)

group_1
group_2
```

Then, I will perform both a t-test and a wilcoxan rank sum test and compare the p-value produced by each.

```{r}
t.test(group_1, group_2)

wilcox.test(group_1, group_2)

```

The p-value for the t.test is ~.14 while the p-value for the wilcoxan is ~.31. Both are making a type 2 error: failing to rejet the null hypothesis when it is false.

Now, I will repeat this process 1000 times, and assess the rate of type 1 and type 2 errors for each test for this sample with a difference of means of 1.

```{r}
set.seed(3)

big.matrix.1 = replicate(1000, rnorm(5, mean = 0))
big.matrix.2 = replicate(1000, rnorm(5, mean = 1))

t_x = function(abc) {
  t.test(x = big.matrix.1[,abc], y = big.matrix.2[,abc])$p.value
}

p_vals.t = lapply(1:1000, t_x)

w_x = function(def) {
  wilcox.test(x = big.matrix.1[,def], y = big.matrix.2[,def])$p.value
}

p_vals.w = lapply(1:1000, w_x)

p_vals = data.frame(t=as.matrix(p_vals.t), w=as.matrix(p_vals.w))

ggplot(p_vals, aes(x=unlist(t),y=unlist(w))) + geom_point()

p_vals.tl = p_vals$t < 0.05
sum(p_vals.tl)

p_vals.wl = p_vals.w < 0.05
sum(p_vals.wl)

```

The t-test rejects the null hypothesis 264 times, while the wilcoxan rejects the null hypothesis only 208 times. Both of these numbers are much greater than the 50 that would be expected by chance with a t-test. Because these samples are drawn from populations with a difference of means of 1, these rejections are accurate. Failing to reject the null represents a type 2 error. The t-test performed fewer type 2 errors, because it can take into account the magnitude of the sample items. The wilcoxan, which can only take into account their rank, has less power to detect the differences between these populations.

Now, I will repeat this process with a few different effect sizes (0, 1, 5) and a few different sample sizes (5, 50), and plot the results.

```{r}
set.seed(3)

big_test = function(s, m) {
  big.matrix.1 = replicate(1000, rnorm(s, mean = 0))
  big.matrix.2 = replicate(1000, rnorm(s, mean = m))
  
  t_x = function(abc) {
    t.test(x = big.matrix.1[,abc], y = big.matrix.2[,abc])$p.value
  }
  
  p_vals.t = lapply(1:1000, t_x)
  
  w_x = function(def) {
    wilcox.test(x = big.matrix.1[,def], y = big.matrix.2[,def])$p.value
  }
  
  p_vals.w = lapply(1:1000, w_x)
  
  p_vals = data.frame(t=as.matrix(p_vals.t), w=as.matrix(p_vals.w))
  
  p_vals.tl = p_vals$t < 0.05
  p_vals.tls = sum(p_vals.tl)
  
  p_vals.wl = p_vals.w < 0.05
  p_vals.wls = sum(p_vals.wl)
  
  return(c(p_vals.tls, p_vals.wls))
}

big_test(5, 0)
big_test(50, 0)

big_test(5, 1)
big_test(50, 1)

big_test(5, 5)
big_test(50, 5)

```

As sample size rises, for two samples drawn from the same population, the t-test is not affected. The wilcoxan makes more type 1 errors.

For two samples drawn from populations with different means, increasing sample size provides more power to both tests to detect this real difference in means. Likewise, as the difference between the means gets larger, both tests are able to reject the null hypothesis more often. For a difference of means of 5, both tests reject the null hypothesis every time, even with a small sample size of n=5.

#### 4d

Now, I will examine these tests on two samples. Both will have a mean of 0, but one will have a standard deviation of 1, and one will have a standard deviation of 3 (sd is the square root of variance).

```{r}
set.seed(10)

group_1 = rnorm(5, sd=1)
group_2 = rnorm(5, sd=3)

group_1
group_2

t.test(group_1, group_2, var.equal=FALSE)

t.test(group_1, group_2, var.equal=TRUE)

wilcox.test(group_1, group_2)
```

Both tests do not reject the null, which is correct in this case. It is interesting to note that the p-value for the t-test is higher than it was was when the samples were both drawn from populations with the same variance; however, the wilcoxan's p-value is unchanged.

Providing the assumption that both populations have the same variance (which is not true in our case) makes the p-value lower for these samples. This makes sense intuitively, as you are providing an assumption and gaining power to detect real differences; you are also increasing the chance of type 1 error when you violate that assumption.

Now I will repeat this 1000 times and assess the rates of type 1 error and power.

```{r}
set.seed(320)

big.matrix.1 = replicate(1000, rnorm(5, sd = 1))
big.matrix.2 = replicate(1000, rnorm(5, sd = 3))

t_x = function(abc) {
  t.test(x = big.matrix.1[,abc], y = big.matrix.2[,abc], var.equal=FALSE)$p.value
}

p_vals.t = lapply(1:1000, t_x)

w_x = function(def) {
  wilcox.test(x = big.matrix.1[,def], y = big.matrix.2[,def])$p.value
}

p_vals.w = lapply(1:1000, w_x)

p_vals = data.frame(t=as.matrix(p_vals.t), w=as.matrix(p_vals.w))

p_vals.tl = p_vals$t < 0.05
sum(p_vals.tl)

p_vals.wl = p_vals.w < 0.05
sum(p_vals.wl)

print('Example p-values, unequal variance:')
p_vals.t[1:5]

```
The t-test produces type 1 errors more often than the wilcoxan here. It is less robust to increases in variance. This makes sense, as increasing variance affects magnitude of differences between sample items, but not rank.



```{r}
t_x_t = function(abc) {
  t.test(x = big.matrix.1[,abc], y = big.matrix.2[,abc], var.equal=TRUE)$p.value
}

p_vals.t = lapply(1:1000, t_x_t)

p_vals.tl = p_vals$t < 0.05
sum(p_vals.tl)

print('Example p-values, equal variance assumed:')
p_vals.t[1:5]
```

Changing the t-test to assume that variances were equal did not result in any more type 1 errors; the number of the times we rejected the null hypothesis was the same. However, if you examine the p-values for each t-test, the p-values for the equal variances assumption are lower in each case.

I am surprised, because I would assume that by chance, you would occassionaly see a p-value which is above 0.05 in the unequal variance assumption dip below 0.05 in the equal variance assumption. However, after testing a handful of seeds, I do not see this effect.

### Problem 5
**Imagine you have collected two groups of observations, one with n = 3 and one with n = 4 from some populations with unknown distributions. You want to test for a difference in location of these two distributions, so you perform a Wilcoxon Rank Sum test. Assume that there are no ties in your observations.**

#### 5a
**What is the minimum p-value you could possibly obtain with this test?**

There are 7 total observations, so 7 total ranks. There are two samples. There are [7 choose 2], or 21 different ways to order these observations. That means the minimum p-value obtainable is 1/21, or ~0.048.

**What is the maximum p-value you could possibly obtain with this test?**

The maximum p-value obtainable is 1. If you had your most extreme Tx value, all possible permutations of observations would fall at or below that value. This would occur when there is no overalp in the values of the groups, such that all of one group is ranked above all of the other group.

#### 5b
Ties are given mid rank. **What happens to the test statistic distribution when a tie is present?**  
Intuitively, ties are given the same rank. This means that the sum of ranks for multiple permutations of the data will be the same. This means that the Tx distribution will be narrowed.

For large N, the test statistic distribution can be modeled approximately as the normal distribution. **If we used this approximation for a WRST with ties present, how should we adjust the normal approximation of the test statistic distribution to account for the ties?**

We should decrease our standard deviation for the normal approximation, to account for the decreased dispersion observed when ties are present. I'm not sure how to calculate the value that we should decrease standard deviation by.

### Problem 6
#### 6a
**Write a function in R to calculate TW RS the Wilcoxon Rank Sum test statistic.**

The function takes in two lists, which are groups of values. It ranks these as a pool, then calculates the sum of ranks for group a. It returns this sum as the test statistic.

```{r}
diy.wilcox = function(a, b) {
  value = c(a, b)
  group = c(rep('a', length(a)), rep('b', length(b)))
  rnk = rank(value)
  
  results = data.frame(value, group, rnk)
  
  x_group = results[which(group=='a'), ]
  tx = sum(x_group$rnk)
  
  return(tx)
  
}

diy.wilcox(c(220,310,12,13),c(1030,230,20))

```

#### 6b

Now, I want to enable the function to return the p-value of the Wilcoxan Rank Sum test. Since the test statistic is a sum, it should be normally distributed and I will use a normal approximation. The two-sided p-value will be the probability that two samples drawn from the same population would have that large of a test statistic by chance.

First, I will add a flag to my function to indicate whether I want the test statistic ('t') or the p-value ('p').

```{r}
diy.wilcox = function(a, b, f) {
  value = c(a, b)
  group = c(rep('a', length(a)), rep('b', length(b)))
  rnk = rank(value)
  
  results = data.frame(value, group, rnk)
  
  x_group = results[which(group=='a'), ]
  tx = sum(x_group$rnk)
  
  center = (length(a)*(length(a) + length(b) + 1))/2 # Formulas from reference (https://www.stat.auckland.ac.nz/~wild/ChanceEnc/Ch10.wilcoxon.pdf)
  
  var = sqrt((length(a)*length(b)*(length(a) + length(b) + 1))/12) # Formulas from reference
  
  z = (tx-center)/var
  
  p = 2 * pnorm(z)
  
  if(f == 't') {return(tx)}
  
  if(f == 'p') {return(p)}
  
}

diy.wilcox(c(1,2,3,4,5),c(100,200,300,400,500),'p')

wilcox.test(c(1,2,3,4,5),c(100,200,300,400,500))

```

Comparing my function to the built in r function, the results are similar. The difference may be due to my use of the normal approximation on a small sample.